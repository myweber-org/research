def remove_duplicates(data_list):
    """
    Remove duplicate entries from a list while preserving order.
    Returns a new list with unique elements.
    """
    seen = set()
    unique_list = []
    for item in data_list:
        if item not in seen:
            seen.add(item)
            unique_list.append(item)
    return unique_list

def clean_numeric_strings(data_list):
    """
    Convert string representations of numbers to integers where possible.
    Non-convertible items remain unchanged.
    """
    cleaned = []
    for item in data_list:
        if isinstance(item, str) and item.isdigit():
            cleaned.append(int(item))
        else:
            cleaned.append(item)
    return cleaned

if __name__ == "__main__":
    sample_data = [1, 2, 2, 3, "4", "4", "five", 3, 1]
    print("Original:", sample_data)
    
    unique_data = remove_duplicates(sample_data)
    print("After deduplication:", unique_data)
    
    cleaned_data = clean_numeric_strings(unique_data)
    print("After numeric cleaning:", cleaned_data)import numpy as np

def remove_outliers_iqr(data, column):
    """
    Remove outliers from a specified column using the Interquartile Range method.
    
    Args:
        data (np.ndarray): Input data array
        column (int): Index of column to clean
    
    Returns:
        np.ndarray: Data with outliers removed
    """
    if not isinstance(data, np.ndarray):
        raise TypeError("Input data must be a numpy array")
    
    if column >= data.shape[1]:
        raise IndexError("Column index out of bounds")
    
    column_data = data[:, column]
    
    q1 = np.percentile(column_data, 25)
    q3 = np.percentile(column_data, 75)
    iqr = q3 - q1
    
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    
    mask = (column_data >= lower_bound) & (column_data <= upper_bound)
    
    return data[mask]

def calculate_statistics(data):
    """
    Calculate basic statistics for the cleaned data.
    
    Args:
        data (np.ndarray): Input data array
    
    Returns:
        dict: Dictionary containing mean, median, and std
    """
    if data.size == 0:
        return {"mean": 0, "median": 0, "std": 0}
    
    return {
        "mean": np.mean(data, axis=0),
        "median": np.median(data, axis=0),
        "std": np.std(data, axis=0)
    }

def validate_data(data):
    """
    Validate input data for cleaning operations.
    
    Args:
        data: Input data to validate
    
    Returns:
        bool: True if data is valid, False otherwise
    """
    if data is None:
        return False
    
    if isinstance(data, np.ndarray):
        return data.size > 0
    
    try:
        arr = np.array(data)
        return arr.size > 0
    except:
        return False

def process_dataset(data, columns_to_clean):
    """
    Process dataset by cleaning multiple columns.
    
    Args:
        data (np.ndarray): Input data array
        columns_to_clean (list): List of column indices to clean
    
    Returns:
        tuple: (cleaned_data, statistics_dict)
    """
    if not validate_data(data):
        raise ValueError("Invalid input data")
    
    cleaned_data = data.copy()
    
    for column in columns_to_clean:
        if 0 <= column < cleaned_data.shape[1]:
            cleaned_data = remove_outliers_iqr(cleaned_data, column)
    
    stats = calculate_statistics(cleaned_data)
    
    return cleaned_data, stats
import pandas as pd
import numpy as np

def remove_missing_values(df, threshold=0.5):
    """
    Remove columns with missing values exceeding threshold percentage.
    """
    missing_percent = df.isnull().sum() / len(df)
    columns_to_drop = missing_percent[missing_percent > threshold].index
    return df.drop(columns=columns_to_drop)

def fill_missing_with_median(df, columns=None):
    """
    Fill missing values with column median.
    """
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns
    
    df_filled = df.copy()
    for col in columns:
        if col in df.columns:
            df_filled[col] = df[col].fillna(df[col].median())
    return df_filled

def detect_outliers_iqr(df, column):
    """
    Detect outliers using IQR method.
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

def cap_outliers(df, column, method='iqr'):
    """
    Cap outliers to specified bounds.
    """
    df_capped = df.copy()
    if method == 'iqr':
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
    elif method == 'percentile':
        lower_bound = df[column].quantile(0.01)
        upper_bound = df[column].quantile(0.99)
    else:
        raise ValueError("Method must be 'iqr' or 'percentile'")
    
    df_capped[column] = df_capped[column].clip(lower=lower_bound, upper=upper_bound)
    return df_capped

def standardize_columns(df, columns=None):
    """
    Standardize numeric columns to zero mean and unit variance.
    """
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns
    
    df_standardized = df.copy()
    for col in columns:
        if col in df.columns:
            mean = df[col].mean()
            std = df[col].std()
            if std > 0:
                df_standardized[col] = (df[col] - mean) / std
    return df_standardized

def clean_dataset(df, missing_threshold=0.5, outlier_method='iqr'):
    """
    Comprehensive data cleaning pipeline.
    """
    df_clean = remove_missing_values(df, threshold=missing_threshold)
    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
    df_clean = fill_missing_with_median(df_clean, columns=numeric_cols)
    
    for col in numeric_cols:
        if col in df_clean.columns:
            df_clean = cap_outliers(df_clean, col, method=outlier_method)
    
    df_clean = standardize_columns(df_clean, columns=numeric_cols)
    return df_clean
import numpy as np
import pandas as pd
from scipy import stats

def remove_outliers_iqr(dataframe, column, multiplier=1.5):
    """
    Remove outliers from a DataFrame column using the IQR method.
    
    Parameters:
    dataframe (pd.DataFrame): Input DataFrame
    column (str): Column name to process
    multiplier (float): IQR multiplier for outlier detection
    
    Returns:
    pd.DataFrame: DataFrame with outliers removed
    """
    if column not in dataframe.columns:
        raise ValueError(f"Column '{column}' not found in DataFrame")
    
    q1 = dataframe[column].quantile(0.25)
    q3 = dataframe[column].quantile(0.75)
    iqr = q3 - q1
    
    lower_bound = q1 - multiplier * iqr
    upper_bound = q3 + multiplier * iqr
    
    filtered_df = dataframe[(dataframe[column] >= lower_bound) & 
                           (dataframe[column] <= upper_bound)]
    
    return filtered_df.copy()

def zscore_normalize(dataframe, columns=None):
    """
    Normalize specified columns using z-score normalization.
    
    Parameters:
    dataframe (pd.DataFrame): Input DataFrame
    columns (list): List of column names to normalize. If None, normalize all numeric columns.
    
    Returns:
    pd.DataFrame: DataFrame with normalized columns
    """
    if columns is None:
        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns
        columns = list(numeric_cols)
    
    normalized_df = dataframe.copy()
    
    for col in columns:
        if col not in normalized_df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame")
        
        if normalized_df[col].dtype not in [np.number]:
            raise TypeError(f"Column '{col}' must be numeric for normalization")
        
        mean_val = normalized_df[col].mean()
        std_val = normalized_df[col].std()
        
        if std_val > 0:
            normalized_df[col] = (normalized_df[col] - mean_val) / std_val
        else:
            normalized_df[col] = 0
    
    return normalized_df

def minmax_normalize(dataframe, columns=None, feature_range=(0, 1)):
    """
    Normalize specified columns using min-max normalization.
    
    Parameters:
    dataframe (pd.DataFrame): Input DataFrame
    columns (list): List of column names to normalize. If None, normalize all numeric columns.
    feature_range (tuple): Desired range of transformed data
    
    Returns:
    pd.DataFrame: DataFrame with normalized columns
    """
    if columns is None:
        numeric_cols = dataframe.select_dtypes(include=[np.number]).columns
        columns = list(numeric_cols)
    
    normalized_df = dataframe.copy()
    min_range, max_range = feature_range
    
    for col in columns:
        if col not in normalized_df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame")
        
        if normalized_df[col].dtype not in [np.number]:
            raise TypeError(f"Column '{col}' must be numeric for normalization")
        
        min_val = normalized_df[col].min()
        max_val = normalized_df[col].max()
        
        if max_val > min_val:
            normalized_df[col] = ((normalized_df[col] - min_val) / (max_val - min_val)) * (max_range - min_range) + min_range
        else:
            normalized_df[col] = min_range
    
    return normalized_df

def detect_skewed_columns(dataframe, threshold=0.5):
    """
    Detect columns with significant skewness.
    
    Parameters:
    dataframe (pd.DataFrame): Input DataFrame
    threshold (float): Absolute skewness threshold for detection
    
    Returns:
    dict: Dictionary with column names and their skewness values
    """
    skewed_cols = {}
    numeric_cols = dataframe.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        skewness = stats.skew(dataframe[col].dropna())
        if abs(skewness) > threshold:
            skewed_cols[col] = skewness
    
    return skewed_cols

def log_transform(dataframe, columns):
    """
    Apply log transformation to specified columns.
    
    Parameters:
    dataframe (pd.DataFrame): Input DataFrame
    columns (list): List of column names to transform
    
    Returns:
    pd.DataFrame: DataFrame with transformed columns
    """
    transformed_df = dataframe.copy()
    
    for col in columns:
        if col not in transformed_df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame")
        
        if transformed_df[col].dtype not in [np.number]:
            raise TypeError(f"Column '{col}' must be numeric for log transformation")
        
        min_val = transformed_df[col].min()
        if min_val <= 0:
            transformed_df[col] = np.log1p(transformed_df[col] - min_val + 1)
        else:
            transformed_df[col] = np.log(transformed_df[col])
    
    return transformed_df
import pandas as pd
import numpy as np

def remove_outliers_iqr(df, column):
    """
    Remove outliers from a specified column using the Interquartile Range method.
    
    Parameters:
    df (pd.DataFrame): Input DataFrame
    column (str): Column name to process
    
    Returns:
    pd.DataFrame: DataFrame with outliers removed
    """
    if column not in df.columns:
        raise ValueError(f"Column '{column}' not found in DataFrame")
    
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    
    return filtered_df.reset_index(drop=True)

def calculate_summary_statistics(df):
    """
    Calculate basic summary statistics for numeric columns.
    
    Parameters:
    df (pd.DataFrame): Input DataFrame
    
    Returns:
    pd.DataFrame: Summary statistics
    """
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    summary = df[numeric_cols].agg(['count', 'mean', 'std', 'min', 'max'])
    
    return summary.T

def validate_dataframe(df, required_columns=None):
    """
    Validate DataFrame structure and content.
    
    Parameters:
    df (pd.DataFrame): DataFrame to validate
    required_columns (list): List of required column names
    
    Returns:
    dict: Validation results
    """
    validation_results = {
        'is_valid': True,
        'missing_columns': [],
        'null_counts': {},
        'data_types': {}
    }
    
    if required_columns:
        missing = [col for col in required_columns if col not in df.columns]
        if missing:
            validation_results['is_valid'] = False
            validation_results['missing_columns'] = missing
    
    for column in df.columns:
        null_count = df[column].isnull().sum()
        if null_count > 0:
            validation_results['null_counts'][column] = null_count
        
        validation_results['data_types'][column] = str(df[column].dtype)
    
    return validation_results

def example_usage():
    """
    Example demonstrating the usage of data cleaning functions.
    """
    np.random.seed(42)
    
    data = {
        'id': range(1, 101),
        'value': np.random.normal(100, 15, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100)
    }
    
    df = pd.DataFrame(data)
    
    print("Original DataFrame shape:", df.shape)
    
    validation = validate_dataframe(df, required_columns=['id', 'value', 'category'])
    print("Validation results:", validation)
    
    cleaned_df = remove_outliers_iqr(df, 'value')
    print("Cleaned DataFrame shape:", cleaned_df.shape)
    
    summary_stats = calculate_summary_statistics(cleaned_df)
    print("Summary statistics:")
    print(summary_stats)
    
    return cleaned_df

if __name__ == "__main__":
    result_df = example_usage()